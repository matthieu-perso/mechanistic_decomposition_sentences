{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed7d850f-58ad-48e0-ae6f-d58cc6ddf5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pandas scikit-learn transformers sentence-transformers matplotlib umap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7204c9ad-9e8c-429e-b007-350b1c2dfe88",
   "metadata": {},
   "source": [
    "# Reconstruction \n",
    "\n",
    "We test if from the probes we are able to reconstruct. We test for both the linear and non linear ones. \n",
    "\n",
    "1. We get an embedding for a sentence \n",
    "2. Get the probe for each token. We add them together.\n",
    "3. We mean summarize them\n",
    "4. Compare that to the base representation using R^2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "392e69a2-f3db-479c-83e0-7190421284b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccb13f89-e7b7-4b50-99df-843b4778109b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.pkl\t\t       output_log_prev.txt\n",
      "dict_probes.pkl\t\t       probe_factorization.ipynb\n",
      "matthieu_test.ipynb\t       probing_word_level.ipynb\n",
      "matthieunlp\t\t       requirements.txt\n",
      "mlp_classifier.joblib\t       train_models.py\n",
      "multiobject_world_model.ipynb  trained_models\n",
      "one_hot_encoder.joblib\t       vikram_probing_word_level.ipynb\n",
      "output_log.txt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8ca9a85-26ce-4840-8097-ebe21240e729",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 14] Bad address",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 2\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 14] Bad address"
     ]
    }
   ],
   "source": [
    "with open('dataset.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "082e742c-52a4-4039-a759-1b8c503289e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>embedding</th>\n",
       "      <th>pos</th>\n",
       "      <th>dep</th>\n",
       "      <th>position</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>[tensor(-0.2899), tensor(0.2611), tensor(-0.30...</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The Fulton County Grand Jury said Friday an in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fulton</td>\n",
       "      <td>[tensor(-0.1788), tensor(0.6752), tensor(-0.56...</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>compound</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>The Fulton County Grand Jury said Friday an in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>County</td>\n",
       "      <td>[tensor(-0.2251), tensor(-0.0033), tensor(-0.2...</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>compound</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>The Fulton County Grand Jury said Friday an in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Grand</td>\n",
       "      <td>[tensor(-0.5069), tensor(0.2878), tensor(0.179...</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>The Fulton County Grand Jury said Friday an in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jury</td>\n",
       "      <td>[tensor(0.2416), tensor(0.0862), tensor(0.1484...</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>The Fulton County Grand Jury said Friday an in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435043</th>\n",
       "      <td>outside</td>\n",
       "      <td>[tensor(0.6717), tensor(0.2565), tensor(0.3406...</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "      <td>26</td>\n",
       "      <td>19999</td>\n",
       "      <td>Of course, most toilets are Eastern style -- a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435044</th>\n",
       "      <td>toilets</td>\n",
       "      <td>[tensor(0.5431), tensor(-0.2458), tensor(-0.18...</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>conj</td>\n",
       "      <td>27</td>\n",
       "      <td>19999</td>\n",
       "      <td>Of course, most toilets are Eastern style -- a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435045</th>\n",
       "      <td>--</td>\n",
       "      <td>[tensor(0.3646), tensor(0.1068), tensor(0.0683...</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punct</td>\n",
       "      <td>28</td>\n",
       "      <td>19999</td>\n",
       "      <td>Of course, most toilets are Eastern style -- a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435046</th>\n",
       "      <td>inside</td>\n",
       "      <td>[tensor(0.6022), tensor(-0.3015), tensor(-0.66...</td>\n",
       "      <td>ADV</td>\n",
       "      <td>advmod</td>\n",
       "      <td>29</td>\n",
       "      <td>19999</td>\n",
       "      <td>Of course, most toilets are Eastern style -- a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435047</th>\n",
       "      <td>.</td>\n",
       "      <td>[tensor(0.7513), tensor(0.1579), tensor(0.1346...</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punct</td>\n",
       "      <td>30</td>\n",
       "      <td>19999</td>\n",
       "      <td>Of course, most toilets are Eastern style -- a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>435048 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word                                          embedding    pos  \\\n",
       "0           The  [tensor(-0.2899), tensor(0.2611), tensor(-0.30...    DET   \n",
       "1        Fulton  [tensor(-0.1788), tensor(0.6752), tensor(-0.56...  PROPN   \n",
       "2        County  [tensor(-0.2251), tensor(-0.0033), tensor(-0.2...  PROPN   \n",
       "3         Grand  [tensor(-0.5069), tensor(0.2878), tensor(0.179...    ADJ   \n",
       "4          Jury  [tensor(0.2416), tensor(0.0862), tensor(0.1484...  PROPN   \n",
       "...         ...                                                ...    ...   \n",
       "435043  outside  [tensor(0.6717), tensor(0.2565), tensor(0.3406...    ADJ   \n",
       "435044  toilets  [tensor(0.5431), tensor(-0.2458), tensor(-0.18...   NOUN   \n",
       "435045       --  [tensor(0.3646), tensor(0.1068), tensor(0.0683...  PUNCT   \n",
       "435046   inside  [tensor(0.6022), tensor(-0.3015), tensor(-0.66...    ADV   \n",
       "435047        .  [tensor(0.7513), tensor(0.1579), tensor(0.1346...  PUNCT   \n",
       "\n",
       "             dep  position  sentence_id  \\\n",
       "0            det         0            0   \n",
       "1       compound         1            0   \n",
       "2       compound         2            0   \n",
       "3           amod         3            0   \n",
       "4          nsubj         4            0   \n",
       "...          ...       ...          ...   \n",
       "435043      amod        26        19999   \n",
       "435044      conj        27        19999   \n",
       "435045     punct        28        19999   \n",
       "435046    advmod        29        19999   \n",
       "435047     punct        30        19999   \n",
       "\n",
       "                                                 sentence  \n",
       "0       The Fulton County Grand Jury said Friday an in...  \n",
       "1       The Fulton County Grand Jury said Friday an in...  \n",
       "2       The Fulton County Grand Jury said Friday an in...  \n",
       "3       The Fulton County Grand Jury said Friday an in...  \n",
       "4       The Fulton County Grand Jury said Friday an in...  \n",
       "...                                                   ...  \n",
       "435043  Of course, most toilets are Eastern style -- a...  \n",
       "435044  Of course, most toilets are Eastern style -- a...  \n",
       "435045  Of course, most toilets are Eastern style -- a...  \n",
       "435046  Of course, most toilets are Eastern style -- a...  \n",
       "435047  Of course, most toilets are Eastern style -- a...  \n",
       "\n",
       "[435048 rows x 7 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ad2bd9-4d53-4fbd-8c29-c442f2f6c54a",
   "metadata": {},
   "source": [
    "# Reconstructing the Token from the Main probes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39139607-b677-4df6-803e-eaa53d3496e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37ff738-46ff-42bc-aec9-d648b50a2d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24853cfe-7c3d-4578-b0f3-f3b823be54eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class NonlinearProbe(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "        \n",
    "class AdaptiveSoftmaxProbe(nn.Module):\n",
    "    def __init__(self, input_dim, n_classes):\n",
    "        super().__init__()\n",
    "        cutoffs = [min(1000, n_classes - 1)]\n",
    "        if n_classes > 10000:\n",
    "            cutoffs.append(min(10000, n_classes - 2))\n",
    "        self.adaptive_softmax = nn.AdaptiveLogSoftmaxWithLoss(\n",
    "            in_features=input_dim,\n",
    "            n_classes=n_classes,\n",
    "            cutoffs=cutoffs,\n",
    "            div_value=4.0\n",
    "        )\n",
    "\n",
    "    def forward(self, x, target=None):\n",
    "        if target is not None:\n",
    "            return self.adaptive_softmax(x, target)\n",
    "        else:\n",
    "            return self.adaptive_softmax.log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06cdcbfe-0b70-40b1-930b-49a22c805dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROBE_DIR = 'trained_models/20250329_183025'\n",
    "\n",
    "with open('trained_models/20250329_183025/dep_model.pkl', 'rb') as file:\n",
    "    dep_probe = pickle.load(file)\n",
    "\n",
    "with open('trained_models/20250329_233721/pos_model.pkl', 'rb') as file:\n",
    "    pos_probe = pickle.load(file)\n",
    "\n",
    "with open('trained_models/20250331_022918//word_model.pkl', 'rb') as file:\n",
    "    word_probe = pickle.load(file)\n",
    "\n",
    "with open('trained_models/20250329_183025/dep_nonlinear.pkl', 'rb') as file:\n",
    "    dep_nl_probe = pickle.load(file)\n",
    "\n",
    "with open('trained_models//20250329_183025/pos_nonlinear.pkl', 'rb') as file:\n",
    "    pos_nl_probe = pickle.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9808cdaf-dfc3-4f4c-9688-d5e7f8bc18e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word_model.linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd47e4e-f660-44b3-bb2e-562d7424eaf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8303a570-db24-4958-93fb-f24a1ac8a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3b6b65-dc04-4fc9-9038-39edbaf36cb6",
   "metadata": {},
   "source": [
    "# Test if the tokens sum to the full thing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b7a5be9c-cdc7-4ab3-9e9b-d9db20c7ca07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00028937435\n",
      "0.00028937435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Input should have at least 1 dimension i.e. satisfy `len(x.shape) > 0`, got scalar `array(0.00028937, dtype=float32)` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(mean_np)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(minilm_np)\n\u001b[0;32m---> 48\u001b[0m     mse \u001b[38;5;241m=\u001b[39m \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminilm_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: sentence_id,\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m\"\u001b[39m: sentence_text,\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosine_similarity\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos_sim,\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m: mse\n\u001b[1;32m     55\u001b[0m     })\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# === Results & Summary\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:565\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \n\u001b[1;32m    517\u001b[0m \u001b[38;5;124;03mRead more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;124;03m0.825...\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    563\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred, sample_weight, multioutput)\n\u001b[1;32m    564\u001b[0m _, y_true, y_pred, sample_weight, multioutput \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 565\u001b[0m     \u001b[43m_check_reg_targets_with_floating_dtype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m )\n\u001b[1;32m    569\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    570\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m _average((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:198\u001b[0m, in \u001b[0;36m_check_reg_targets_with_floating_dtype\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, xp)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Ensures that y_true, y_pred, and sample_weight correspond to the same\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03mregression task.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m    correct keyword.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m dtype_name \u001b[38;5;241m=\u001b[39m _find_matching_floating_dtype(y_true, y_pred, sample_weight, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m--> 198\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# _check_reg_targets does not accept sample_weight as input.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Convert sample_weight's data type separately to match dtype_name.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:104\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03mTo reduce redundancy when calling `_find_matching_floating_dtype`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    correct keyword.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred, multioutput, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m--> 104\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    106\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:472\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_consistent_length\u001b[39m(\u001b[38;5;241m*\u001b[39marrays):\n\u001b[1;32m    455\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that all arrays have consistent first dimensions.\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \n\u001b[1;32m    457\u001b[0m \u001b[38;5;124;03m    Checks whether all objects in arrays have the same shape or length.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;124;03m    >>> check_consistent_length(a, b)\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    473\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:472\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_consistent_length\u001b[39m(\u001b[38;5;241m*\u001b[39marrays):\n\u001b[1;32m    455\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that all arrays have consistent first dimensions.\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \n\u001b[1;32m    457\u001b[0m \u001b[38;5;124;03m    Checks whether all objects in arrays have the same shape or length.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;124;03m    >>> check_consistent_length(a, b)\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m [\u001b[43m_num_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    473\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:399\u001b[0m, in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 399\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    400\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput should have at least 1 dimension i.e. satisfy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    401\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`len(x.shape) > 0`, got scalar `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m         )\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;66;03m# Check that shape is returning an integer or default to len\u001b[39;00m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;66;03m# Dask dataframes may not return numeric shape[0] value\u001b[39;00m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], numbers\u001b[38;5;241m.\u001b[39mIntegral):\n",
      "\u001b[0;31mTypeError\u001b[0m: Input should have at least 1 dimension i.e. satisfy `len(x.shape) > 0`, got scalar `array(0.00028937, dtype=float32)` instead."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# === Device & Model ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "model.eval()\n",
    "\n",
    "sentence_groups = list(df.groupby(\"sentence_id\"))[:1000]  # limit to first 1000\n",
    "\n",
    "results = []\n",
    "\n",
    "for sentence_id, group in tqdm(sentence_groups, desc=\"Processing\"):\n",
    "    token_embeddings = torch.stack(group['embedding'].tolist())  # shape: (num_tokens, dim)\n",
    "    sentence_text = group['sentence'].iloc[0]\n",
    "\n",
    "    mean_embedding = token_embeddings.mean(dim=0)\n",
    "\n",
    "    inputs = tokenizer(sentence_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        \"\"\" \n",
    "        output = model(**inputs)\n",
    "        token_embeddings = output.last_hidden_state[0]  # shape: [seq_len, hidden_dim]\n",
    "        attention_mask = inputs['attention_mask'][0].unsqueeze(-1)  # shape: [seq_len, 1]\n",
    "        masked_embeddings = token_embeddings * attention_mask  # zero out [PAD] tokens\n",
    "        mean_embedding = masked_embeddings.sum(dim=0) / attention_mask.sum()\n",
    "        \"\"\"\n",
    "        \n",
    "        mean_embedding = token_embeddings.mean()\n",
    "        minilm_embedding = mean_embedding.to(device)\n",
    "\n",
    "    mean_np = mean_embedding.cpu().numpy()\n",
    "    minilm_np = minilm_embedding.cpu().numpy()\n",
    "    \n",
    "    cos_sim = cosine_similarity(mean_np.reshape(1, -1), minilm_np.reshape(1, -1))[0, 0]\n",
    "\n",
    "    print(mean_np)\n",
    "    print(minilm_np)\n",
    "    mse = mean_squared_error(mean_np, minilm_np)\n",
    "\n",
    "    results.append({\n",
    "        \"sentence_id\": sentence_id,\n",
    "        \"sentence\": sentence_text,\n",
    "        \"cosine_similarity\": cos_sim,\n",
    "        \"mse\": mse\n",
    "    })\n",
    "\n",
    "# === Results & Summary\n",
    "results_df = pd.DataFrame(results)\n",
    "mean_cosine = results_df['cosine_similarity'].mean()\n",
    "mean_mse = results_df['mse'].mean()\n",
    "\n",
    "print(f\"\\nAverage Cosine Similarity: {mean_cosine:.4f}\")\n",
    "print(f\"Average MSE: {mean_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac0eea2-a9da-47be-852b-74567fc6d27d",
   "metadata": {},
   "source": [
    "# We can now test \n",
    "\n",
    "1. For a given sentence \n",
    "2. Get the full sentence embedding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e2d3b4-3010-4db4-8cd4-ee372a790d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "pos_weights = pos_model.linear.weight.detach().cpu().numpy()\n",
    "dep_weights = dep_model.linear.weight.detach().cpu().numpy()\n",
    "\n",
    "def get_probe_weights_for_word(word, tokenizer):\n",
    "    word_tokens = tokenizer.encode(word)\n",
    "    \n",
    "    if len(word_tokens) == 0:\n",
    "        raise ValueError(f\"Word '{word}' not found in the tokenizer's vocabulary.\")\n",
    "    \n",
    "    word_idx = list(le_word.classes_).index(word)\n",
    "    probe_weights = word_model.linear.weight[word_idx].detach().cpu().numpy()\n",
    "    return probe_weights / len(word_tokens)\n",
    "\n",
    "\n",
    "def compute_sentence_embeddings(df, tokenizer):\n",
    "    sentence_embeddings = []\n",
    "    \n",
    "    for sentence_id, group in df.groupby('sentence_id'):\n",
    "        weighted_sum = torch.zeros(384)\n",
    "        total_num_tokens = 0\n",
    "\n",
    "        for _, row in group.iterrows():\n",
    "            word = row['word'].lower()\n",
    "            total_num_tokens += 1\n",
    "            probe_weight = get_probe_weights_for_word(word, tokenizer)\n",
    "            weighted_sum += probe_weight\n",
    "\n",
    "        # get POS\n",
    "        pos = row['pos']\n",
    "        pos_idx = list(le_pos.classes_).index(pos)\n",
    "        pos_probe_weight = pos_weights[pos_idx]\n",
    "\n",
    "        # get DEP\n",
    "        dep = row['dep']\n",
    "        dep_idx = list(le_dep.classes_).index(dep)\n",
    "        dep_probe_weight = dep_weights[dep_idx]\n",
    "\n",
    "        full_probe_embedding = (weighted_sum.numpy() / total_num_tokens) + pos_probe_weight + dep_probe_weight\n",
    "     \n",
    "        sentence_embeddings.append({\n",
    "            'sentence_id': sentence_id,\n",
    "            'sentence': group['sentence'].iloc[0],\n",
    "            'probe_embedding': full_probe_embedding\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(sentence_embeddings)\n",
    "\n",
    "summed_embeddings = compute_sentence_embeddings(sample_df, tokenizer)\n",
    "summed_embeddings = summed_embeddings.drop_duplicates('sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b6c804-b7b6-4325-a4ff-0416c699ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = summed_embeddings['sentence'].values.tolist()\n",
    "inputs = tokenizer(all_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "\n",
    "input_ids = inputs['input_ids']\n",
    "mask = (input_ids != tokenizer.pad_token_id) & (input_ids != tokenizer.cls_token_id) & (input_ids != tokenizer.sep_token_id)\n",
    "masked_embeddings = embeddings[mask]\n",
    "sentence_embeddings = torch.mean(embeddings, dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
